---
title: "The stochastic Lotka-Volterra model, simulation and maximum likelihood inference"
author: "Vignette by Pierre Gloaguen, based on a joint work with Sylvain Le Corff"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    number_sections: yes
    toc: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, message = FALSE, include = FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  eval = TRUE,
  collapse = TRUE,
  comment = NA,
  cache = T,
  fig.align = "center",
  fig.width = 8,
  fig.height = 6
)
theme_set(theme_gray() +
            theme(
              panel.border = element_rect(colour = "black", 
                                          fill = rgb(0, 0, 0, 0)),
              panel.grid = element_line(linetype = 2),
              legend.background = element_blank(), 
              text = element_text(family = "Symbola", size = 24),
              strip.background = element_rect(color = "black", 
                                              fill = "lightgoldenrod1")))
```


# The partially observed stochastic Lotka-Voltera Model

## Hidden dynamics

We consider a bivariate predator-prey dynamical system.
More precisely, we consider a bivariate stochastic process $\mathbf{X}(t) = (X_1(t), X_2(t))$ modelling the abundance of a prey (the first component of $X(t)$) and of its predator (the second component). 
We assume that $\mathbf{X}(t)$ is the solution of the following stochastic differntial equation:
$$\text{d} \mathbf{X}(t) = \alpha_\theta(\mathbf{X}(t)) \text{d} t + \Gamma_\theta(\mathbf{X}(t))\text{d} \mathbf{W}(t),~\mathbf{X}(0) \sim \mathcal{N}_2(\mu_0, \Sigma_0)$$
where:
$$
\begin{array}{rrcl}
\alpha_\theta: & \mathbb{R}^2 & \mapsto & \mathbb{R}^2\\
& x = (x_1, x_2)^T & \mapsto & \alpha_\theta(x) = \begin{pmatrix} x_1( a_{10} - a_{11}x_1 - a_{12}x_2)\\  x_2(-a_{20} + a_{21}x_1 - a_{22}x_2) \end{pmatrix}\\
\Gamma_\theta: & \mathbb{R}^2 & \mapsto & \mathcal{M}_{2 \times 2}\\
& x = (x_1, x_2)^T & \mapsto & \Gamma_\theta(x) = \begin{pmatrix}
\Gamma_{11}x_1 & \Gamma_{12}x_1\\
\Gamma_{21}x_2 & \Gamma_{22}x_2
\end{pmatrix}
\end{array}
$$

The hidden parameter $\theta$ is then given by the set of $\mathbf{a}_1, \mathbf{a}_2, \Gamma$ which are parameters lying respectively in $\mathbb{R}^3_+, \mathbb{R}^3_+$, and $\mathbb{R}^4$.

## Observation process

In our context, the hidden dynamics are not directly observed, but only partially, through an observation process $\mathbf{Y}(k) (0\leq k\leq n)$, observed only at times $t_0,\dots t_n$.

The process is defined as:
$$\mathbf{Y}(k) = \begin{pmatrix}
q_1X_1(t_k)\text{e}^{\varepsilon_{1,k}}\\
q_2X_2(t_k)\text{e}^{\varepsilon_{2,k}}
\end{pmatrix}$$

where $\mathbf{\varepsilon}_k \overset{ind}{\sim} \mathcal{N}_2(\mathbf{0}, \Sigma_{obs})$, with an unknown parameter $\Sigma_{obs}$. The parameters $q_1$ and $q_2$ are assumed to be known.

# Simulation using the `LotkaVoltR` package

In the following, the parameters were chosen as follow. 
The names of the parameters refer to equations of the previous section.

## Chosen parameters in this vignette

```{r chosen_parameters, eval = T}
rm(list = ls()) # Cleaning environment

# Dynamics parameters

a1 <- c(12, 0.05, 1) # Prey parameters
a2 <- c(2, 0.2, 0.1) # Predator parameters
Gamma <- matrix(c(0.5, 0.1, 0.1, 0.2), nrow = 2) # Diffusion parameters
mu0 <- c(50,20) # Mean of the inital distribution
Sigma0 <- diag(1, 2) # Variance of the initial distribution

# Observation process parameters

Sigma_obs <- matrix(c(0.01, 0.005, 0.005, 0.01), ncol = 2) # Observation noise
q_values <- c(0.2, 0.3) # Known q values

# Model creation

library(LotkaVoltR) # Dedicated library
# Creation of a model instance in R
POLV_model <- POLV_create(a1 = a1, a2 = a2, gam = Gamma, mu0 = mu0, 
                          sigma0 = Sigma0, cov = Sigma_obs, qs = q_values)
POLV_model
model_base <- LV_create(a1 = a1, a2 = a2, gam = Gamma, mu0 = mu0, 
                          sigma0 = Sigma0)
```

## Simulation of a Lotka-Voltera model

Once the model is instantiate (through the `POLV_create` function used above), it is straigforward to simulate from it, using an Euler scheme.

```{r sim_true_abundances, eval = TRUE}
# Simulation parameters

simulation_times <- seq(from = 0, by = 1e-6, # Simulation time step, small!!
                        length.out = 3*10^6 + 1)

# If the simulation must be done at small time steps, the output can be thinned
selection <- seq(1, length(simulation_times), length.out = 301)
set.seed(333)
simulated_process <- POLV_simulate(POLV_model,
                                   times = simulation_times, 
                                   selection = selection)
observation_times <- simulation_times[selection]
simulated_data <- data.frame(value = as.numeric(simulated_process[, -5]),
                             animal = factor(rep(rep(c("prey", "pred"), 
                                                     each = length(selection)),
                                                 2)),
                             status = factor(rep(c("Observed", "True"), 
                                                 each = 2 * length(selection))),
                             t = rep(observation_times, 4))
```

The `simulated_data` can then be be plotted, either with respect to time:

```{r plot_LV_time, eval = TRUE, echo = FALSE}
ggplot(data = simulated_data) + 
  aes(x = t, y = value, col = animal, linetype = status) +
  geom_line(size = 0.5)  + 
  labs(x = "Time", y = "Value",
       col = "Animal", linetype = "Signal")
```

or in the observation space:

```{r plot_LV_space, echo = F, eval = T}
simulated_data %>% 
  filter(status == "True") %>% # Only the observed
  spread(key = animal, value = value) %>% 
  ggplot() + 
  aes(x = prey, y = pred) +
  geom_path(aes(alpha = t), col = "darkgreen")  + 
  geom_point() +
  labs(x = "Observed number of preys", y = "Observed number of predators",
       alpha = "Time")
```

# Evaluating likelihood when the process is observed without error

Thanks to the continuous importance sampling algorithm, unbiased estimates of the 
transtion density can be estimated for each set of parameters etween each point.

More formally, as a Markov process, the process $\mathbf{X}(t)$ is defined by the density of the random variable $X(t+\delta)\vert X(t)$, which is assumed to have a density $q_\delta(X(t),\cdot)$.
For observed values $\mathbf{x}(t_0),\dots \mathbf{x}(t_n)$ recorded at times $t_0,t1 = t_0+\delta_1,\dots t_n = t_{n-1} + \delta_n$, the likelihood is given by:

$$L(\mathbf{x}_{t_0:t_n};\theta) = \prod_{k = 1}^n q_{\delta_{k}}(\mathbf{x}_{t_{k - 1}}, \mathbf{x}_{t_{k}})$$

Unfortunately, this quantity is intractable. The CIS algorithm, though, can be performed to obtained unbiased estimates of this quantity

## Obtaining unbiased density estimate for one transition

Thanks to the function `POLV_density`, given a model, two (exact) observations, a time lag and a Monte Carlo effort, it is straighforward to obtain an estimate.

```{r one_sample, echo = T, eval = T}
# Considering the first two observations

ind_obs0 <- 1 # Observation indexes
ind_obs1 <- 2 
x0 <- simulated_process[ind_obs0, c("X1", "X2")] # Initial number of prey/preds  
x1 <- simulated_process[ind_obs1, c("X1", "X2")] # Subsequent number 
delta_lag <- observation_times[ind_obs1] - observation_times[ind_obs0] # time lag

# The density is then esimated
set.seed(333)
estimated_density <- POLV_density(POLV_model = POLV_model, # For the given model
                                  x0 = x0, # Starting point  
                                  xT = x1, # Ending point
                                  time_lag = delta_lag, # Duration between obs
                                  n_samples = 500 # Monte Carlo effort
                                  )
estimated_density
```

We can ensure that this estimator converges to a stable value by replicating the procedure:

```{r several_samples, eval = T}
library(parallel)
n_rep <- 10000
all_estimates <- mclapply(1:n_rep, 
                 function(i){
                   RNGkind()
                   set.seed(i)
                   POLV_density(POLV_model = POLV_model, # For the given model
                                  x0 = x0, # Starting point  
                                  xT = x1, # Ending point
                                  time_lag = delta_lag, # Duration between obs
                                  n_samples = 50 # Monte Carlo effort
                                  )
                 },
  mc.cores = detectCores()) %>% 
  unlist() 
```

By plotting the evolution of the empirical means, we ensure the convergence:

```{r plotting_empirical_mean, eval = T}
all_estimates %>% 
  tibble(x = .) %>% 
  rowid_to_column(var = "ind") %>% 
  ggplot() + 
  aes(x = ind, y = cumsum(x) / (1:n_rep)) + 
  geom_point() + 
  labs(x = "Number of replicates", y = "Empirical mean of estimates") 
```

## Ensuring the matching with Euler estimate for small time lags

The only way to ensure that the estimate is reliable (in this generic context) is
to compare it when $\delta$ is really small, with the Euler estimate.

In the following graph, the horizontal line shows the Euler density approximation 
for $\delta = 10^{-8}$.

```{r euler_comparison, echo = F, eval = T}
set.seed(12)
my_length <- 1001
new_sim_times <- seq(0, by = 1e-8, length.out = my_length)
new_selection <- 1:my_length
new_obs <- POLV_simulate(POLV_model,
                         times = new_sim_times, 
                         selection = new_selection)
ind_obs0 <- sample(1:(my_length - 1), size = 1) # Observation indexes
ind_obs1 <- ind_obs0 + 1
x0 <- new_obs[ind_obs0, c("X1", "X2")] # Initial number of prey/preds  
x1 <- new_obs[ind_obs1, c("X1", "X2")]
delta_lag <- new_sim_times[ind_obs1] - new_sim_times[ind_obs0]
euler_density <- POLV_model$euler_density(x0, x1, delta_lag)
library(parallel)
n_rep <- 1e3
all_estimates <- mclapply(1:n_rep, 
                 function(i){
                   RNGkind()
                   set.seed(i)
                   POLV_density(POLV_model = POLV_model, # For the given model
                                  x0 = x0, # Starting point  
                                  xT = x1, # Ending point
                                  time_lag = delta_lag, # Duration between obs
                                  n_samples = 3000 # Monte Carlo effort
                                  )
                 },
  mc.cores = detectCores()) %>% 
  unlist()
all_estimates %>% 
  tibble(x = .) %>% 
  rowid_to_column(var = "ind") %>% 
  mutate(empirical_mean = cumsum(x) / ind) %>% 
  slice(seq(1, n_rep, length.out = min(n_rep, 1e4 + 1))) %>% 
  ggplot() + 
  aes(x = ind, y = empirical_mean) + 
  geom_point() + 
  labs(x = "Number of replicates", y = "Empirical mean of estimates") +
  geom_hline(yintercept = POLV_model$euler_density(x0, x1, delta_lag))
```

# Particle filter

## Using the proposal model, and coding the PF in R

The particle filtering is performed using a proposal model, which has the same parameters has the normal model, plus some details.

```{r proposal_model, eval = TRUE}
parameters_list <- list(a1 = a1, a2 = a2, mu0 = mu0,
                        sigma0 = Sigma0, RWC = diag(0.005, 2), 
                        qs = q_values, cov = Sigma_obs,
                        wD = 1, wO = 1,
                        gam = Gamma)
proposal_model <- PLV_create(parameters_list)
```

```{r get_normed_weights}
get_normed_weights <- function(unnormed_weights){
  return(unnormed_weights / sum(unnormed_weights))
}
```


```{r getting_first_particles}
set.seed(123)
observations <- simulated_process[, c("Y1", "Y2")]
y0 <- observations[1, ]
n_particles <- 1e2
initial_particles <- proposal_model$simX0(n_particles, y0)
p0 <- proposal_model$propDens0(initial_particles, y0)
q0 <- proposal_model$modelDens0(initial_particles)
w0 <- get_normed_weights(q0 / p0)
colSums(w0 * initial_particles)
simulated_process[1,]
initial_particles %>% 
  as_tibble() %>% 
  rename(X1 = V1, X2 = V2) %>% 
  mutate(weight = w0) %>% 
  ggplot() +
  aes(x = X1, y = X2) + 
  geom_point(mapping = aes(alpha = weight)) + 
  geom_point(data = as.data.frame(simulated_process[1,,drop =F]), col = "red")
```


```{r getting_weighted_particle_set}
# n_obs <- nrow(observations)
# tic()
# n_particles <- 1e2
# initial_particles <- proposal_model$simX0(n_particles, y0)
# p0 <- proposal_model$propDens0(initial_particles, y0)
# q0 <- proposal_model$modelDens0(initial_particles)
# particle_set <- array(dim = c(n_particles, 2, n_obs))
# particle_set[,,1] <- initial_particles
# weights <- matrix(nrow = n_particles, ncol = n_obs)
# weights[, 1] <- w0
# for(i in 2:n_obs){
#   # print(paste("Iteration", i))
#   kept_particles <- sample(1:n_particles, 
#                            size = n_particles,
#                            replace = T,
#                            prob = weights[, i - 1])
#   old_particles <- particle_set[kept_particles,, i - 1]
#   current_y <- observations[i, ]
#   time_lag <- simulated_process[i, "t"] - simulated_process[i - 1, "t"]
#   new_particles <- proposal_model$simNextX(old_particles, current_y, time_lag)
#   proposal_density <- proposal_model$propDens(old_particles, new_particles, 
#                                               current_y, time_lag)
#   transition_density <- proposal_model$transDens(old_particles, new_particles, 
#                                  time_lag, 100, 200)
#   observation_density <- proposal_model$obsDens(new_particles, current_y)
#   weights[, i] <- get_normed_weights(observation_density * transition_density / 
#                                        proposal_density) 
#   particle_set[,, i] <- new_particles
# }
# toc()
```


```{r get_effective_sample_size}
get_effective_sample_size <- function(w) sum(w)^2 / sum(w^2)
ess <- apply(weights, 2, get_effective_sample_size)
plot(ess)
summary(ess)
```


```{r representing_result}
# all_results <- map_dfr(1:n_obs, function(i){
#   tibble(X1 = particle_set[, 1, i],
#          X2 = particle_set[, 2, i],
#          weight = weights[, i],
#          time  = observation_times[i])
# })
# 
# empirical_mean <- all_results %>% 
#   mutate(time = as.factor(time)) %>% 
#   group_by(time) %>% 
#   summarise(X1 = sum(X1 * weight),
#             X2 = sum(X2 * weight))
# ggplot(mapping = aes(x = X1, y = X2)) +
#   geom_point(data = as.data.frame(simulated_process)) +
#   geom_path(data = empirical_mean, col = "blue")
# ggplot(mapping = aes(x = X1, y = X2)) +
#   geom_point(data = all_results, mapping = aes(alpha = weight), col = "blue") +
#   geom_point(data = as.data.frame(simulated_process))
```

## Using the class `ParticleFilter`

```{r PF_create}
library(tictoc)
observations <- simulated_process[, c("Y1", "Y2")]
tic()
particle_filter <- PF_create(parameters_list, t(observations),
                             observation_times, 2e2,
                             n_euler_skel = 30)
smoothing_exp <- particle_filter$runSmoothing()
save(observations, observation_times, smoothing_exp, 
     particle_filter,
     file = "smoothed_tracking_LV.RData") 
toc()
```

```{r plot_results_PF}
n_obs <- nrow(observations)
my_particle_set <- particle_filter$get_particles()
my_particle_weights <- particle_filter$get_weights()
results <- map_dfr(1:n_obs, function(i){
  tibble(X1 = my_particle_set[, 1, i],
         X2 = my_particle_set[, 2, i],
         weight = my_particle_weights[, i],
         time  = observation_times[i])
})
empirical_mean <- results %>% 
  mutate(time = as.factor(time)) %>%
  group_by(time) %>%
  summarise(X1 = sum(X1 * weight),
            X2 = sum(X2 * weight))
smoothing_mean <- smoothing_exp %>% 
  as_tibble() %>% 
  rename(X1 = V1, X2 = V2) %>% 
  mutate(method = "Smoothing est.")
observed_values <- simulated_process %>% 
  as.data.frame() %>% 
  select(Y1, Y2) %>% 
  mutate(Y1 = Y1 / q_values[1],
         Y2 = Y2 / q_values[2]) %>%
  rename(X1 = Y1, X2 = Y2) %>% 
  mutate(method = "Observed")
true_values <- simulated_process %>% 
  as.data.frame() %>% 
  select(X1, X2) %>% 
  mutate(method = "Truth")

concatened_results <- bind_rows(observed_values,
                                true_values,
                                smoothing_mean)
ggplot(concatened_results) +
  aes(x = X1, y = X2, color = method, linetype = method) +
  geom_point() +
  geom_path() +
  theme(legend.position = "none") + 
  facet_wrap(~method) +
  labs(x = "Number of preys", y = "Number of predators")
```



